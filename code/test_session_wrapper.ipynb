{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-21.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-21.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.7/42.7 MB\u001b[0m \u001b[31m176.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-21.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "from aind_dynamic_foraging_behavior_video_analysis.kinematics.tongue_kinematics_utils import get_session_name_from_path, plot_keypoint_confidence_analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_tongue_dfs(predictions_csv_path: Path, data_root: Path, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Runs the full pipeline for one session and returns the NWB object, \n",
    "    annotated tongue kinematics, and aggregated tongue movements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions_csv_path : Path\n",
    "        Path to the predictions CSV (LP_csv).\n",
    "    data_root : Path\n",
    "        Root folder containing behavior_<‚Ä¶> session subfolders.\n",
    "    tolerance : float, optional\n",
    "        Lick-kinematics matching tolerance (default 0.01).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple : (nwb, tongue_kin, tongue_movs)\n",
    "        nwb: NWBFile object with annotated licks/trials added\n",
    "        tongue_kin: frame-level annotated tongue kinematics (DataFrame)\n",
    "        tongue_movs: movement-level aggregated tongue movements (DataFrame)\n",
    "    \"\"\"\n",
    "    # === Imports inside so function is self-contained ===\n",
    "    from aind_dynamic_foraging_behavior_video_analysis.kinematics.kinematics_nwb_utils import get_nwb_file\n",
    "    from aind_dynamic_foraging_behavior_video_analysis.kinematics.tongue_kinematics_utils import (\n",
    "        load_keypoints_from_csv, find_behavior_videos_folder,\n",
    "        integrate_keypoints_with_video_time, mask_keypoint_data,\n",
    "        kinematics_filter, segment_movements_trimnans,\n",
    "        annotate_trials_in_kinematics, annotate_licks_in_kinematics,\n",
    "        assign_movements_to_licks, aggregate_tongue_movements,\n",
    "        add_lick_metadata_to_movements, get_session_name_from_path\n",
    "    )\n",
    "    import aind_dynamic_foraging_data_utils.nwb_utils as nwb_utils\n",
    "    from aind_dynamic_foraging_basic_analysis.licks import annotation\n",
    "\n",
    "    # === 1) Session detection ===\n",
    "    lp_csv = predictions_csv_path\n",
    "    session_id = get_session_name_from_path(str(lp_csv))\n",
    "    print(f\"\\n=== Generating tongue data for session: {session_id} ===\")\n",
    "    print(f\"Predictions CSV: {lp_csv}\")\n",
    "\n",
    "    # === 2) Load keypoints ===\n",
    "    kps = load_keypoints_from_csv(str(lp_csv))\n",
    "    print(f\"Loaded keypoints: {len(kps)} raw dataframes\")\n",
    "\n",
    "    # === 3) Locate synced video CSV ===\n",
    "    videos_folder = find_behavior_videos_folder(str(data_root / session_id))\n",
    "    if videos_folder is None:\n",
    "        raise FileNotFoundError(f\"Videos folder not found for session {session_id}\")\n",
    "    video_csv = Path(videos_folder) / \"bottom_camera.csv\"\n",
    "    if not video_csv.exists():\n",
    "        raise FileNotFoundError(f\"Expected video CSV at {video_csv}\")\n",
    "    print(f\"Found video CSV: {video_csv}\")\n",
    "\n",
    "    # === 4) Sync keypoints to video time ===\n",
    "    kps_trim, _ = integrate_keypoints_with_video_time(str(video_csv), kps)\n",
    "    print(f\"Synced keypoints\")\n",
    "\n",
    "    # === 5) Tongue movement segmentation ===\n",
    "    tongue_masked = mask_keypoint_data(kps_trim, 'tongue_tip_center', confidence_threshold=0.90)\n",
    "    tongue_filtered = kinematics_filter(tongue_masked, cutoff_freq=50, filter_order=4, filter_kind='cubic')\n",
    "    tongue_seg = segment_movements_trimnans(tongue_filtered, max_dropped_frames=10)\n",
    "    print(f\"Segmented {tongue_seg['movement_id'].nunique()} unique movements\")\n",
    "\n",
    "    # === 6) Load NWB and annotate ===\n",
    "    nwb = get_nwb_file(session_id)\n",
    "    nwb.df_events = nwb_utils.create_events_df(nwb)\n",
    "    nwb.df_trials = nwb_utils.create_df_trials(nwb)\n",
    "    nwb.df_licks = annotation.annotate_licks(nwb)\n",
    "    print(f\"NWB load: {len(nwb.df_trials)} trials, {len(nwb.df_licks)} licks\")\n",
    "\n",
    "    tongue_annot = annotate_trials_in_kinematics(tongue_seg, nwb.df_trials)\n",
    "    tongue_kin = annotate_licks_in_kinematics(tongue_annot, nwb.df_licks, tolerance=tolerance)\n",
    "    nwb.df_licks = assign_movements_to_licks(tongue_kin, nwb.df_licks)\n",
    "    print(\"Annotated kinematics with trials & licks\")\n",
    "\n",
    "    # === 7) Aggregate movements ===\n",
    "    tongue_movs = aggregate_tongue_movements(tongue_kin, kps_trim)\n",
    "    tongue_movs = add_lick_metadata_to_movements(\n",
    "        tongue_movs, nwb.df_licks, fields=['cue_response','rewarded','event']\n",
    "    )\n",
    "    print(f\"Aggregated movements DF shape: {tongue_movs.shape}\")\n",
    "\n",
    "    return nwb, tongue_kin, tongue_movs, kps_trim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_percentile_movements(\n",
    "    df: pd.DataFrame,\n",
    "    metric_col: str,\n",
    "    percentiles: list = [0, 0.25, 0.5, 0.75, 1.0]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return movement_ids and corresponding metric values at specified percentiles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain 'movement_id' and the metric column.\n",
    "    metric_col : str\n",
    "        Name of the numeric column to sort and index into.\n",
    "    percentiles : list of float\n",
    "        Values between 0 and 1 for desired percentiles.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: ['movement_id', metric_col, 'percentile']\n",
    "    \"\"\"\n",
    "    if metric_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{metric_col}' not found\")\n",
    "    series = df[['movement_id', metric_col]].dropna()\n",
    "    if not pd.api.types.is_numeric_dtype(series[metric_col]):\n",
    "        raise ValueError(f\"Column '{metric_col}' must be numeric\")\n",
    "    if any(p < 0 or p > 1 for p in percentiles):\n",
    "        raise ValueError(\"Percentiles must be in [0, 1]\")\n",
    "\n",
    "    sorted_df = series.sort_values(metric_col).reset_index(drop=True)\n",
    "    N = len(sorted_df)\n",
    "    if N == 0:\n",
    "        return pd.DataFrame(columns=['movement_id', metric_col, 'percentile'])\n",
    "\n",
    "    rows = []\n",
    "    for p in percentiles:\n",
    "        idx = int(round(p * (N - 1)))\n",
    "        rows.append({\n",
    "            'movement_id': int(sorted_df.loc[idx, 'movement_id']),\n",
    "            metric_col: sorted_df.loc[idx, metric_col],\n",
    "            'percentile': p\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def plot_movement_tiles_scatter(\n",
    "    tongue_segmented: pd.DataFrame,\n",
    "    movement_ids: list,\n",
    "    x_col: str,\n",
    "    y_col: str,\n",
    "    labels: list = None,\n",
    "    color: str = 'gray',\n",
    "    s: int = 5,\n",
    "    title: str = None,\n",
    "    return_fig=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot scatter of any two kinematic columns for a given list of movement_ids.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tongue_segmented : pd.DataFrame\n",
    "        Frame-level data with at least 'movement_id', x_col, y_col.\n",
    "    movement_ids : list\n",
    "        Movement IDs to plot (one subplot per movement).\n",
    "    x_col : str\n",
    "        Column in tongue_segmented to plot on x-axis.\n",
    "    y_col : str\n",
    "        Column in tongue_segmented to plot on y-axis.\n",
    "    labels : list, optional\n",
    "        List of strings or values to annotate each subplot (same length as movement_ids).\n",
    "    color : str\n",
    "        Point color for scatter.\n",
    "    s : int\n",
    "        Point size for scatter.\n",
    "    title : str\n",
    "        Title of figure\n",
    "    \"\"\"\n",
    "    n = len(movement_ids)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n * 2, 2), sharex=True, sharey=True)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Global axis limits\n",
    "    all_x, all_y = [], []\n",
    "    for mid in movement_ids:\n",
    "        df = tongue_segmented[tongue_segmented['movement_id'] == mid]\n",
    "        df = df[[x_col, y_col]].dropna()\n",
    "        all_x.extend(df[x_col])\n",
    "        all_y.extend(df[y_col])\n",
    "    if not all_x:\n",
    "        raise RuntimeError(\"No valid movements found to plot.\")\n",
    "    xlim = (min(all_x), max(all_x))\n",
    "    ylim = (min(all_y), max(all_y))\n",
    "\n",
    "    for i, (ax, mid) in enumerate(zip(axes, movement_ids)):\n",
    "        df = tongue_segmented[tongue_segmented['movement_id'] == mid]\n",
    "        df = df[[x_col, y_col]].dropna()\n",
    "\n",
    "        if len(df) < 1:\n",
    "            ax.scatter([0], [0], s=10, color='black')\n",
    "        else:\n",
    "            ax.scatter(df[x_col], df[y_col], s=s, color=color)\n",
    "\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.set_xlabel(x_col, fontsize=7)\n",
    "        if ax == axes[0]:\n",
    "            ax.set_ylabel(y_col, fontsize=7)\n",
    "\n",
    "        if labels is not None:\n",
    "            ax.set_title(str(labels[i]), fontsize=8)\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=10)\n",
    "    else:\n",
    "        plt.suptitle(f\"{y_col} vs {x_col}\", fontsize=10)\n",
    "    \n",
    "    # plt.suptitle(f\"{y_col} vs {x_col}\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    if return_fig:\n",
    "        return fig\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_tongue_movement_quality(\n",
    "    kps_raw: dict,\n",
    "    tongue_kins: pd.DataFrame,\n",
    "    tongue_movs: pd.DataFrame,\n",
    "    nwb,\n",
    "    save_dir: str,\n",
    "    percentiles: list = [0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0],\n",
    "    pred_csv=None \n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze and visualize tongue movement quality for a single session.\n",
    "\n",
    "    Saves figures and key summary stats in a session-specific folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tongue_kins : pd.DataFrame\n",
    "        Frame-level kinematics data.\n",
    "    tongue_movs : pd.DataFrame\n",
    "        Movement-level kinematics data (one row per movement).\n",
    "    nwb : NWB object with df_licks.\n",
    "    pred_csv : str\n",
    "        Path to the prediction CSV (used to infer session name).\n",
    "    save_dir : str\n",
    "        Directory where results will be saved.\n",
    "    percentiles : list\n",
    "        Percentiles to sample for movement quality plots.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------\n",
    "    # Setup & Folders\n",
    "    # ----------------\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    session_id = os.path.basename(save_dir)\n",
    "\n",
    "    print(f\"Analyzing session: {session_id}\")\n",
    "    \n",
    "    # ----------------\n",
    "    # Confidence figure\n",
    "    # ----------------\n",
    "    keypt = 'tongue_tip_center'  # Example, can be parameterized in wrapper\n",
    "    plot_keypoint_confidence_analysis(\n",
    "        keypoint_dfs=kps_raw,\n",
    "        keypt=keypt,\n",
    "        save_dir=save_dir,\n",
    "        save_figures=True\n",
    "        )\n",
    "        \n",
    "    # ----------------\n",
    "    # Lick Coverage\n",
    "    # ----------------\n",
    "    total_licks = len(nwb.df_licks)\n",
    "    with_mov = nwb.df_licks['nearest_movement_id'].notna().sum()\n",
    "    coverage_pct = 100 * with_mov / total_licks if total_licks else np.nan\n",
    "\n",
    "    lick_movs = tongue_movs[tongue_movs['has_lick']]\n",
    "    lick_times = nwb.df_licks['timestamps']\n",
    "    has_mov = nwb.df_licks['nearest_movement_id'].notna()\n",
    "    covered_times = lick_times[has_mov]\n",
    "    missed_times = lick_times[~has_mov]\n",
    "\n",
    "    # ----------------\n",
    "    # Lick Coverage Figure\n",
    "    # ----------------\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(14, 8))\n",
    "    parent_gs = fig.add_gridspec(2, 1, height_ratios=[1, 1])\n",
    "    gs_top = parent_gs[0].subgridspec(1, 3, width_ratios=[0.5, 6, 3])\n",
    "    gs_bottom = parent_gs[1].subgridspec(1, 3)\n",
    "\n",
    "    ax_cov = fig.add_subplot(gs_top[0, 0])\n",
    "    ax_raster = fig.add_subplot(gs_top[0, 1])\n",
    "    ax_scat = fig.add_subplot(gs_top[0, 2])\n",
    "    ax_h0 = fig.add_subplot(gs_bottom[0, 0])\n",
    "    ax_h1 = fig.add_subplot(gs_bottom[0, 1])\n",
    "    ax_h2 = fig.add_subplot(gs_bottom[0, 2])\n",
    "\n",
    "    # --- Coverage Bar ---\n",
    "    n_missed = total_licks - with_mov\n",
    "    ax_cov.bar(0, coverage_pct, color='green', label=f'Covered (n={with_mov})')\n",
    "    ax_cov.bar(0, 100 - coverage_pct, bottom=coverage_pct,\n",
    "               color='red', label=f'Missed (n={n_missed})')\n",
    "    ax_cov.set_ylim(0, 100)\n",
    "    ax_cov.set_xticks([])\n",
    "    ax_cov.set_title(\"Lick Coverage (%)\", fontsize=10)\n",
    "    ax_cov.legend(fontsize=7, loc='lower center')\n",
    "\n",
    "    # --- Raster ---\n",
    "    ax_raster.eventplot(\n",
    "        [covered_times, missed_times],\n",
    "        lineoffsets=[1, 0], linelengths=0.8,\n",
    "        colors=['green', 'red']\n",
    "    )\n",
    "    ax_raster.set_yticks([1, 0])\n",
    "    ax_raster.set_yticklabels(['Covered', 'Missed'])\n",
    "    ax_raster.set_xlabel('Time in session (s)')\n",
    "    ax_raster.set_title('Lick coverage over session')\n",
    "\n",
    "    # --- Scatter ---\n",
    "    ax_scat.scatter(lick_movs['duration'], lick_movs['dropped_frames_pct'],\n",
    "                    alpha=0.05, edgecolor='k')\n",
    "    ax_scat.set_xlabel('Duration (s)')\n",
    "    ax_scat.set_ylabel('Dropped Frame %')\n",
    "    ax_scat.set_title('Duration vs Drop%')\n",
    "\n",
    "    # --- Histograms ---\n",
    "    ax_h0.hist(lick_movs['n_datapoints'], bins=30)\n",
    "    ax_h0.set(title='Datapoints')\n",
    "    ax_h1.hist(lick_movs['duration'], bins=30)\n",
    "    ax_h1.set(title='Duration')\n",
    "    ax_h2.hist(lick_movs['dropped_frames_pct'], bins=30)\n",
    "    ax_h2.set(title='Dropped %')\n",
    "\n",
    "    plt.suptitle(f'{session_id}', y=1.02)\n",
    "    fig.savefig(os.path.join(save_dir, \"lick_coverage_summary.png\"), dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ----------------\n",
    "    # Movement Percentile Plots\n",
    "    # ----------------\n",
    "    tongue_kins['time_in_movement'] = (\n",
    "        tongue_kins['time'] -\n",
    "        tongue_kins.groupby('movement_id')['time'].transform('first')\n",
    "    )\n",
    "\n",
    "    percentile_results = {}\n",
    "    for metric_col in ['dropped_frames_n', 'duration']:\n",
    "        sel = select_percentile_movements(tongue_movs, metric_col=metric_col, percentiles=percentiles)\n",
    "        labels = [f\"{int(p*100)}%ile: {val:.2f}\" \n",
    "                  for p, val in zip(sel['percentile'], sel[metric_col])]\n",
    "        percentile_results[metric_col] = dict(zip(sel['percentile'], sel[metric_col]))\n",
    "\n",
    "\n",
    "        fig = plot_movement_tiles_scatter(\n",
    "            tongue_segmented=tongue_kins,\n",
    "            movement_ids=sel['movement_id'].tolist(),\n",
    "            x_col='time_in_movement',\n",
    "            y_col='x',\n",
    "            labels=labels,\n",
    "            color='gray',\n",
    "            title=metric_col,\n",
    "            return_fig=True\n",
    "        )\n",
    "        fig.savefig(os.path.join(save_dir, f\"{metric_col}_tiles.png\"), dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # ----------------\n",
    "    # Save Everything to JSON\n",
    "    # ----------------\n",
    "    results_dict = {\n",
    "        \"session_id\": os.path.basename(save_dir),\n",
    "        \"pred_csv\": str(pred_csv) if pred_csv else None,\n",
    "        \"total_licks\": int(total_licks),\n",
    "        \"licks_with_movement\": int(with_mov),\n",
    "        \"coverage_pct\": float(coverage_pct),\n",
    "        \"percentiles\": percentile_results\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(save_dir, \"tongue_quality_stats.json\"), \"w\") as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Finished analysis for {session_id}. Results saved to {save_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_analysis(\n",
    "    pred_csv_list, \n",
    "    data_root, \n",
    "    save_root, \n",
    "    percentiles=None, \n",
    "    extract_clips=True  \n",
    "):\n",
    "    \"\"\"\n",
    "    Run analysis for multiple sessions in batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred_csv_list : list of str or Path\n",
    "        List of prediction CSV paths (one per session).\n",
    "    data_root : str or Path\n",
    "        Root folder where behavior_<...> session folders live.\n",
    "    save_root : str or Path\n",
    "        Root folder to save all analysis outputs.\n",
    "    percentiles : list, optional\n",
    "        Percentiles for movement quality plots (default: [0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0]).\n",
    "    extract_clips : bool, optional\n",
    "        Whether to extract example video clips for each session (default: True).\n",
    "    \"\"\"\n",
    "    percentiles = percentiles or [0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "    save_root = Path(save_root)\n",
    "    save_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    error_log = []\n",
    "\n",
    "    for pred_csv in pred_csv_list:\n",
    "        pred_csv = Path(pred_csv)\n",
    "        session_id = get_session_name_from_path(str(pred_csv))\n",
    "\n",
    "        print(f\"\\nüîπ Starting analysis for: {session_id}\")\n",
    "        session_save_dir = os.path.join(save_root, session_id)\n",
    "        os.makedirs(session_save_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            # ---- 1) Generate DFs ----\n",
    "            nwb, tongue_kins, tongue_movs, kps_raw = generate_tongue_dfs(pred_csv, data_root)\n",
    "\n",
    "            # ---- 1a) Save intermediate data ----\n",
    "            intermediate_folder = os.path.join(session_save_dir, \"intermediate_data\")\n",
    "            os.makedirs(intermediate_folder, exist_ok=True)\n",
    "\n",
    "            # Save tongue_kins and tongue_movs\n",
    "            tongue_kins.to_parquet(os.path.join(intermediate_folder, \"tongue_kins.parquet\"))\n",
    "            tongue_movs.to_parquet(os.path.join(intermediate_folder, \"tongue_movs.parquet\"))\n",
    "\n",
    "            # Save each df in kps_raw dict\n",
    "            for key, df in kps_raw.items():\n",
    "                df.to_parquet(os.path.join(intermediate_folder, f\"kps_raw_{key}.parquet\"))\n",
    "\n",
    "            # Save selected NWB dfs\n",
    "            nwb.df_licks.to_parquet(os.path.join(intermediate_folder, \"nwb_df_licks.parquet\"))\n",
    "            nwb.df_trials.to_parquet(os.path.join(intermediate_folder, \"nwb_df_trials.parquet\"))\n",
    "            nwb.df_events.to_parquet(os.path.join(intermediate_folder, \"nwb_df_events.parquet\"))\n",
    "\n",
    "            # ---- 2) Run analysis ----\n",
    "            analyze_tongue_movement_quality(\n",
    "                kps_raw=kps_raw,\n",
    "                tongue_kins=tongue_kins,\n",
    "                tongue_movs=tongue_movs,\n",
    "                nwb=nwb,\n",
    "                save_dir=session_save_dir,\n",
    "                percentiles=percentiles,\n",
    "                pred_csv=pred_csv\n",
    "            )\n",
    "\n",
    "            # ---- 3) Optionally extract example clips ----\n",
    "            if extract_clips:\n",
    "                try:\n",
    "                    extract_example_clips_for_session(\n",
    "                        session_id, \n",
    "                        save_root,  # analysis_root\n",
    "                        data_root\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not extract clips for {session_id}: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå Error in {session_id}: {repr(e)}\"\n",
    "            print(error_msg)\n",
    "            error_log.append(error_msg)\n",
    "            continue  # Move to the next session\n",
    "\n",
    "    # ---- Print & Save Error Log ----\n",
    "    if error_log:\n",
    "        log_file = save_root / \"batch_error_log.txt\"\n",
    "        with open(log_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(error_log))\n",
    "        print(f\"\\n‚ö†Ô∏è Completed with errors. See log: {log_file}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Batch analysis completed successfully for all sessions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example script:\n",
    "# # Path to the predictions CSV (from the processed folder)\n",
    "# pred_csv = Path(\"/root/capsule/data/behavior_751004_2024-12-23_14-19-57_processed_2025-07-10_06-34-56/pred_outputs/video_preds/bottom_camera_predictions.csv\")\n",
    "\n",
    "# # Root folder where the raw behavior_<...> session folders live\n",
    "# data_root = Path(\"/root/capsule/data\")\n",
    "\n",
    "# # save dir\n",
    "# save_root = '/root/capsule/scratch/session_analysis/'\n",
    "\n",
    "# nwb, tongue_kins, tongue_movs, kps_raw = generate_tongue_dfs(pred_csv, data_root)\n",
    "\n",
    "\n",
    "# # ----------------\n",
    "# # Generate Session Folder Name\n",
    "# # ----------------\n",
    "# session_id = get_session_name_from_path(str(pred_csv))\n",
    "# session_save_dir = os.path.join(save_root, session_id)\n",
    "# os.makedirs(session_save_dir, exist_ok=True)\n",
    "\n",
    "# # ----------------\n",
    "# # Save Intermediate Data (Parquet)\n",
    "# # ----------------\n",
    "# intermediate_folder = os.path.join(session_save_dir, \"intermediate_data\")\n",
    "# os.makedirs(intermediate_folder, exist_ok=True)\n",
    "\n",
    "# # Save tongue_kins and tongue_movs\n",
    "# tongue_kins.to_parquet(os.path.join(intermediate_folder, \"tongue_kins.parquet\"))\n",
    "# tongue_movs.to_parquet(os.path.join(intermediate_folder, \"tongue_movs.parquet\"))\n",
    "\n",
    "# # Save each df in kps_raw dict\n",
    "# for key, df in kps_raw.items():\n",
    "#     df.to_parquet(os.path.join(intermediate_folder, f\"kps_raw_{key}.parquet\"))\n",
    "\n",
    "# # Save selected NWB dfs\n",
    "# nwb.df_licks.to_parquet(os.path.join(intermediate_folder, \"nwb_df_licks.parquet\"))\n",
    "# nwb.df_trials.to_parquet(os.path.join(intermediate_folder, \"nwb_df_trials.parquet\"))\n",
    "# nwb.df_events.to_parquet(os.path.join(intermediate_folder, \"nwb_df_events.parquet\"))\n",
    "\n",
    "# # ----------------\n",
    "# # Run Analysis\n",
    "# # ----------------\n",
    "# analyze_tongue_movement_quality(\n",
    "#     kps_raw=kps_raw,\n",
    "#     tongue_kins=tongue_kins,\n",
    "#     tongue_movs=tongue_movs,\n",
    "#     nwb=nwb,\n",
    "#     save_dir=session_save_dir,\n",
    "#     percentiles=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0],\n",
    "#     pred_csv=pred_csv\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Batch Analysis \n",
    "\n",
    "# save_root = \"/root/capsule/scratch/session_analysis_in_distribution\"\n",
    "# data_root = Path(\"/root/capsule/data\")\n",
    "\n",
    "# pred_csv_list = [\n",
    "#     \"/root/capsule/data/BottomViewPylon1-MIB-2025-02-17/inference/behavior_716325_2024-05-31_10-31-14/bottom_camera.csv\",\n",
    "#     \"/root/capsule/data/BottomViewPylon1-MIB-2025-02-17/inference/behavior_717121_2024-06-15_10-00-58/bottom_camera.csv\",\n",
    "#     \"/root/capsule/data/BottomViewPylon1-MIB-2025-02-17/inference/behavior_717259_2024-06-28_11-17-19/bottom_camera.csv\",\n",
    "#     \"/root/capsule/data/BottomViewPylon1-MIB-2025-02-17/inference/behavior_717263_2024-07-24_10-40-05/bottom_camera.csv\",\n",
    "#     \"/root/capsule/data/BottomViewPylon1-MIB-2025-02-17/inference/behavior_751004_2024-12-20_13-26-07/bottom_camera.csv\",\n",
    "#     \"/root/capsule/data/BottomViewPylon1-MIB-2025-02-17/inference/behavior_751004_2024-12-21_13-28-24/bottom_camera.csv\",\n",
    "#     \"/root/capsule/data/BottomViewPylon1-MIB-2025-02-17/inference/behavior_751004_2024-12-22_13-09-11/bottom_camera.csv\",\n",
    "#     \"/root/capsule/data/BottomViewPylon1-MIB-2025-02-17/inference/behavior_751004_2024-12-23_14-19-57/bottom_camera.csv\"\n",
    "# ]\n",
    "\n",
    "# run_batch_analysis(pred_csv_list, data_root, save_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_root = \"/root/capsule/scratch/session_analysis\"\n",
    "# data_root = Path(\"/root/capsule/data\")\n",
    "\n",
    "# pred_csv_list = [\n",
    "#     \"/root/capsule/data/behavior_751004_2024-12-23_14-19-57_processed_2025-07-10_06-34-56/pred_outputs/video_preds/bottom_camera_predictions.csv\",\n",
    "#     \"/root/capsule/data/behavior_751766_2025-02-11_11-53-32_videoprocessed_2025-07-17/pred_outputs/video_preds/bottom_camera_predictions.csv\",\n",
    "#     \"/root/capsule/data/behavior_754897_2025-03-11_12-07-35_videoprocessed_2025-07-08/pred_outputs/video_preds/bottom_camera_predictions.csv\",\n",
    "#     \"/root/capsule/data/behavior_754897_2025-03-13_11-20-39_videoprocessed_2025-07-17/pred_outputs/video_preds/bottom_camera_predictions.csv\",\n",
    "#     \"/root/capsule/data/behavior_758017_2025-02-04_11-57-33_videoprocessed_2025-07-17/pred_outputs/video_preds/bottom_camera_predictions.csv\",\n",
    "#     \"/root/capsule/data/behavior_761038_2025-04-15_10-24-57_videoprocessed_2025-07-17/pred_outputs/video_preds/bottom_camera_predictions.csv\",\n",
    "#     \"/root/capsule/data/behavior_782394_2025-04-24_12-07-31_videoprocessed_2025-07-17/pred_outputs/video_preds/bottom_camera_predictions.csv\"\n",
    "# ]\n",
    "\n",
    "# run_batch_analysis(pred_csv_list, data_root, save_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test single session on batch analysis\n",
    "\n",
    "# save_root = \"/root/capsule/scratch/session_analysis\" #update save location as needed\n",
    "# data_root = Path(\"/root/capsule/data\")\n",
    "\n",
    "# pred_csv_list = [\n",
    "#     \"/root/capsule/data/behavior_758017_2025-02-04_11-57-33_videoprocessed_2025-07-17/pred_outputs/video_preds/bottom_camera_predictions.csv\"\n",
    "# ]\n",
    "\n",
    "# run_batch_analysis(pred_csv_list, data_root, save_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look into individual session for errors when generating dfs\n",
    "\n",
    "\n",
    "# data_root = Path(\"/root/capsule/data\")\n",
    "\n",
    "# pred_csv = \"/root/capsule/data/behavior_754897_2025-03-11_12-07-35_videoprocessed_2025-07-08/pred_outputs/video_preds/bottom_camera_predictions.csv\"\n",
    "# session_id = get_session_name_from_path(str(pred_csv))\n",
    "\n",
    "# # ---- 1) Generate DFs ----\n",
    "# nwb, tongue_kins, tongue_movs, kps_raw = generate_tongue_dfs(pred_csv, data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aind_dynamic_foraging_behavior_video_analysis.kinematics.video_clip_utils import extract_clips_ffmpeg_after_reencode\n",
    "\n",
    "def find_labeled_video(session_id, data_root):\n",
    "    # Find the labeled video file for a session_id, searching for any folder that starts with session_id\n",
    "    data_root = Path(data_root)\n",
    "    for subdir in data_root.glob(f\"{session_id}*\"):\n",
    "        candidate = subdir / \"pred_outputs\" / \"video_preds\" / \"labeled_videos\" / \"bottom_camera_labeled.mp4\"\n",
    "        if candidate.exists():\n",
    "            return str(candidate)\n",
    "    raise FileNotFoundError(f\"Labeled video not found for {session_id}\")\n",
    "\n",
    "def get_trial_level_df(nwb_df_licks, tongue_movs, nwb_df_trials):\n",
    "    # Aggregate licks per trial\n",
    "    licks_per_trial = nwb_df_licks.groupby('trial').size().rename('lick_count')\n",
    "    # Aggregate coverage per trial: percent of licks with a movement\n",
    "    covered = nwb_df_licks['nearest_movement_id'].notna().groupby(nwb_df_licks['trial']).mean().rename('coverage_pct')\n",
    "    # Lick count in first 10s of each trial\n",
    "    first_licks = []\n",
    "    for trial, row in nwb_df_trials.set_index('trial').iterrows():\n",
    "        start = row['goCue_start_time_in_session']\n",
    "        end = start + 10\n",
    "        licks_in_window = nwb_df_licks[\n",
    "            (nwb_df_licks['trial'] == trial) &\n",
    "            (nwb_df_licks['timestamps'] >= start) &\n",
    "            (nwb_df_licks['timestamps'] < end)\n",
    "        ]\n",
    "        first_licks.append(len(licks_in_window))\n",
    "    first10s_lick_count = pd.Series(first_licks, index=nwb_df_trials['trial'], name='first10s_lick_count')\n",
    "    # Merge with trial info\n",
    "    trial_df = nwb_df_trials.set_index('trial').join([licks_per_trial, covered, first10s_lick_count])\n",
    "    trial_df['lick_count'] = trial_df['lick_count'].fillna(0).astype(int)\n",
    "    trial_df['coverage_pct'] = trial_df['coverage_pct'] * 100\n",
    "    trial_df['first10s_lick_count'] = trial_df['first10s_lick_count'].fillna(0).astype(int)\n",
    "    return trial_df\n",
    "\n",
    "def get_video_time(session_time, tongue_kins):\n",
    "    # Find offset between session time and video time using first row\n",
    "    offset = tongue_kins.iloc[0]['time'] - tongue_kins.iloc[0]['time_in_session']\n",
    "    return session_time + offset\n",
    "\n",
    "def extract_trial_clip(\n",
    "    session_id, trial_row, tongue_kins, video_path, save_dir, pad_s=0.5\n",
    "):\n",
    "    # Get trial start/end in session time\n",
    "    start = trial_row['goCue_start_time_in_session']\n",
    "    # end = trial_row['reward_outcome_time_in_session']\n",
    "    end = start + 10\n",
    "    # Convert to video time\n",
    "    video_start = get_video_time(start, tongue_kins) - pad_s\n",
    "    video_end = get_video_time(end, tongue_kins) + pad_s\n",
    "    clip_length = video_end - video_start\n",
    "\n",
    "    # Use trial number as filename stem\n",
    "    trial_num = trial_row.name if hasattr(trial_row, 'name') else trial_row['trial']\n",
    "    filename_stem = f\"trial_{trial_num}\"\n",
    "    # Call the extract function: timestamps is a list of start times\n",
    "    extract_clips_ffmpeg_after_reencode(\n",
    "        video_path, [video_start], clip_length, save_dir, filename_stems=[filename_stem]\n",
    "    )\n",
    "    print(f\"Saved clip for trial {trial_num} to {save_dir}\")\n",
    "    \n",
    "def extract_example_clips_for_session(session_id, analysis_root, data_root):\n",
    "    # Load data\n",
    "    inter_dir = Path(analysis_root) / session_id / \"intermediate_data\"\n",
    "    tongue_movs = pd.read_parquet(inter_dir / \"tongue_movs.parquet\")\n",
    "    tongue_kins = pd.read_parquet(inter_dir / \"tongue_kins.parquet\")\n",
    "    nwb_df_licks = pd.read_parquet(inter_dir / \"nwb_df_licks.parquet\")\n",
    "    nwb_df_trials = pd.read_parquet(inter_dir / \"nwb_df_trials.parquet\")\n",
    "    # Get trial-level stats\n",
    "    trial_df = get_trial_level_df(nwb_df_licks, tongue_movs, nwb_df_trials)\n",
    "    \n",
    "    # Only consider trials with at least 5 licks\n",
    "    trial_df = trial_df[trial_df['first10s_lick_count'] >= 5]\n",
    "\n",
    "    # # Find trial with highest and lowest coverage (among those with high lick count)\n",
    "    # Top 3 trials by coverage\n",
    "    top3 = trial_df.sort_values(['coverage_pct', 'lick_count'], ascending=[False, False]).head(3)\n",
    "    # Bottom 3 trials by coverage\n",
    "    bottom3 = trial_df.sort_values(['coverage_pct', 'lick_count'], ascending=[True, False]).head(3)\n",
    "\n",
    "    # Find video\n",
    "    video_path = find_labeled_video(session_id, data_root)\n",
    "\n",
    "    # Output dirs\n",
    "    good_dir = Path(analysis_root) / session_id / \"example_clips\" / \"good\"\n",
    "    bad_dir = Path(analysis_root) / session_id / \"example_clips\" / \"bad\"\n",
    "    good_dir.mkdir(exist_ok=True, parents=True)\n",
    "    bad_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Extract top 3 (good) clips\n",
    "    for _, trial_row in top3.iterrows():\n",
    "        extract_trial_clip(session_id, trial_row, tongue_kins, video_path, good_dir, pad_s=0.5)\n",
    "    # Extract bottom 3 (bad) clips\n",
    "    for _, trial_row in bottom3.iterrows():\n",
    "        extract_trial_clip(session_id, trial_row, tongue_kins, video_path, bad_dir, pad_s=0.5)\n",
    "\n",
    "# Example usage:\n",
    "# extract_example_clips_for_session(\"behavior_751004_2024-12-23_14-19-57\", \"/root/capsule/scratch/session_analysis\", \"/root/capsule/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_example_clips_for_session(\"behavior_751004_2024-12-23_14-19-57\", \"/root/capsule/scratch/session_analysis\", \"/root/capsule/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_root = \"/root/capsule/scratch/session_analysis_in_distribution\"\n",
    "data_root = \"/root/capsule/data\"\n",
    "\n",
    "for session_id in os.listdir(analysis_root):\n",
    "    session_path = os.path.join(analysis_root, session_id)\n",
    "    inter_dir = os.path.join(session_path, \"intermediate_data\")\n",
    "    # Check if intermediate data exists and has required files\n",
    "    required_files = [\n",
    "        \"tongue_movs.parquet\",\n",
    "        \"tongue_kins.parquet\",\n",
    "        \"nwb_df_licks.parquet\",\n",
    "        \"nwb_df_trials.parquet\"\n",
    "    ]\n",
    "    if not os.path.isdir(inter_dir) or not all(os.path.isfile(os.path.join(inter_dir, f)) for f in required_files):\n",
    "        print(f\"Skipping {session_id}: missing intermediate data.\")\n",
    "        continue\n",
    "    try:\n",
    "        extract_example_clips_for_session(session_id, analysis_root, data_root)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {session_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python==4.11.0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install moviepy==1.0.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
