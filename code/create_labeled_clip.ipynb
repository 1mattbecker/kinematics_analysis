{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy<2.0.0 in /opt/conda/lib/python3.9/site-packages (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.9/site-packages (from moviepy<2.0.0) (4.63.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from moviepy<2.0.0) (2.0.2)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /opt/conda/lib/python3.9/site-packages (from moviepy<2.0.0) (4.4.2)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /opt/conda/lib/python3.9/site-packages (from moviepy<2.0.0) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from moviepy<2.0.0) (2.28.2)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.9/site-packages (from moviepy<2.0.0) (2.37.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /opt/conda/lib/python3.9/site-packages (from moviepy<2.0.0) (0.1.10)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.9/site-packages (from imageio<3.0,>=2.5->moviepy<2.0.0) (10.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy<2.0.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy<2.0.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy<2.0.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy<2.0.0) (1.26.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"moviepy<2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "# from aind_dynamic_foraging_basic_analysis.licks.lick_analysis import load_nwb\n",
    "import re\n",
    "from matplotlib import colormaps  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints extracted: ['nose_tip', 'jaw', 'tongue_tip_right', 'tongue_tip_center', 'tongue_tip_left', 'pointer_finger_r', 'paw_wrist_r', 'pointer_finger_l', 'paw_wrist_l', 'spout_r', 'spout_l']\n",
      "Video QC: Frame numbers are sequential with no gaps.\n",
      "Video QC: Timing differences are within expected range.\n",
      "keypoint_df trimmed from 2689719 to 2689718\n"
     ]
    }
   ],
   "source": [
    "#Load kinematics data\n",
    "from tongue_kinematics_utils import load_keypoints_from_csv, integrate_keypoints_with_video_time\n",
    "\n",
    "#keypoints\n",
    "keypoint_dfs = load_keypoints_from_csv('/root/capsule/data/BottomViewPylon1-MIB-2025-02-17/inference/behavior_716325_2024-05-31_10-31-14/bottom_camera.csv')\n",
    "\n",
    "#get video timebase for keypoint dataframe\n",
    "keypoint_dfs_trimmed, video_csv_trimmed, keypoint_timebase = integrate_keypoints_with_video_time(\n",
    "    '/root/capsule/data/behavior_716325_2024-05-31_10-31-14/behavior-videos/bottom_camera.csv', \n",
    "    keypoint_dfs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def extract_clips_ffmpeg_after_reencode(input_video_path, timestamps, clip_length, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for idx, start_time in enumerate(timestamps):\n",
    "        end_time = start_time + clip_length\n",
    "        input_basename_ext = os.path.basename(input_video_path)\n",
    "        input_basename, _ = os.path.splitext(input_basename_ext)\n",
    "        output_filename = input_basename + f\"_clip_{idx+1}_{start_time:.2f}s_to_{end_time:.2f}s.mp4\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "        if os.path.isfile(output_path):\n",
    "            continue\n",
    "\n",
    "        command = [\n",
    "            'ffmpeg',\n",
    "            '-ss', str(start_time),\n",
    "            '-i', input_video_path,\n",
    "            '-t', str(clip_length),\n",
    "            '-c', 'copy',             # Copy codec (no re-encoding)\n",
    "            output_path\n",
    "        ]\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Clip saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from moviepy.editor import VideoFileClip\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import os\n",
    "\n",
    "\n",
    "def create_labeled_video(\n",
    "    clip: VideoFileClip,\n",
    "    xs_arr: np.ndarray,\n",
    "    ys_arr: np.ndarray,\n",
    "    mask_array: Optional[np.ndarray] = None,\n",
    "    dotsize: int = 4,\n",
    "    colormap: str = \"cool\",\n",
    "    fps: Optional[float] = None,\n",
    "    filename: str = \"movie.mp4\",\n",
    "    start_time: float = 0.0,\n",
    ") -> None:\n",
    "    \"\"\"Helper function for creating annotated videos.\n",
    "\n",
    "    Args\n",
    "        clip\n",
    "        xs_arr: shape T x n_joints\n",
    "        ys_arr: shape T x n_joints\n",
    "        mask_array: shape T x n_joints; timepoints/joints with a False entry will not be plotted\n",
    "        dotsize: size of marker dot on labeled video\n",
    "        colormap: matplotlib color map for markers\n",
    "        fps: None to default to fps of original video\n",
    "        filename: video file name\n",
    "        start_time: time (in seconds) of video start\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if mask_array is None:\n",
    "        mask_array = ~np.isnan(xs_arr)\n",
    "\n",
    "    n_frames, n_keypoints = xs_arr.shape\n",
    "\n",
    "    # set colormap for each color\n",
    "    colors = make_cmap(n_keypoints, cmap=colormap)\n",
    "\n",
    "    # extract info from clip\n",
    "    nx, ny = clip.size\n",
    "    dur = int(clip.duration - clip.start)\n",
    "    fps_og = clip.fps\n",
    "\n",
    "    # upsample clip if low resolution; need to do this for dots and text to look nice\n",
    "    if nx <= 100 or ny <= 100:\n",
    "        upsample_factor = 2.5\n",
    "    elif nx <= 192 or ny <= 192:\n",
    "        upsample_factor = 2\n",
    "    else:\n",
    "        upsample_factor = 1\n",
    "\n",
    "    if upsample_factor > 1:\n",
    "        clip = clip.resize((upsample_factor * nx, upsample_factor * ny))\n",
    "        nx, ny = clip.size\n",
    "\n",
    "    print(f\"Duration of video [s]: {np.round(dur, 2)}, recorded at {np.round(fps_og, 2)} fps!\")\n",
    "\n",
    "    def seconds_to_hms(seconds):\n",
    "        # Convert seconds to a timedelta object\n",
    "        td = datetime.timedelta(seconds=seconds)\n",
    "\n",
    "        # Extract hours, minutes, and seconds from the timedelta object\n",
    "        hours = td // datetime.timedelta(hours=1)\n",
    "        minutes = (td // datetime.timedelta(minutes=1)) % 60\n",
    "        seconds = td % datetime.timedelta(minutes=1)\n",
    "\n",
    "        # Format the hours, minutes, and seconds into a string\n",
    "        hms_str = f\"{hours:02}:{minutes:02}:{seconds.seconds:02}\"\n",
    "\n",
    "        return hms_str\n",
    "\n",
    "    # add marker to each frame t, where t is in sec\n",
    "    def add_marker_and_timestamps(get_frame, t):\n",
    "        image = get_frame(t * 1.0)\n",
    "        # frame [ny x ny x 3]\n",
    "        frame = image.copy()\n",
    "        # convert from sec to indices\n",
    "        index = int(np.round(t * 1.0 * fps_og))\n",
    "        # ----------------\n",
    "        # markers\n",
    "        # ----------------\n",
    "        for bpindex in range(n_keypoints):\n",
    "            if index >= n_frames:\n",
    "                print(\"Skipped frame {}, marker {}\".format(index, bpindex))\n",
    "                continue\n",
    "            if mask_array[index, bpindex]:\n",
    "                xc = min(int(upsample_factor * xs_arr[index, bpindex]), nx - 1)\n",
    "                yc = min(int(upsample_factor * ys_arr[index, bpindex]), ny - 1)\n",
    "                frame = cv2.circle(\n",
    "                    frame,\n",
    "                    center=(xc, yc),\n",
    "                    radius=dotsize,\n",
    "                    color=colors[bpindex].tolist(),\n",
    "                    thickness=-1,\n",
    "                )\n",
    "        # ----------------\n",
    "        # timestamps\n",
    "        # ----------------\n",
    "        seconds_from_start = t + start_time\n",
    "        time_from_start = seconds_to_hms(seconds_from_start)\n",
    "        idx_from_start = int(np.round(seconds_from_start * 1.0 * fps_og))\n",
    "        text = f\"t={time_from_start}, frame={idx_from_start}\"\n",
    "        # define text info\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.5\n",
    "        font_thickness = 1\n",
    "        # calculate the size of the text\n",
    "        text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]\n",
    "        # calculate the position of the text in the upper-left corner\n",
    "        offset = 6\n",
    "        text_x = offset  # offset from the left\n",
    "        text_y = text_size[1] + offset  # offset from the bottom\n",
    "        # make black rectangle with a small padding of offset / 2 pixels\n",
    "        cv2.rectangle(\n",
    "            frame,\n",
    "            (text_x - int(offset / 2), text_y + int(offset / 2)),\n",
    "            (text_x + text_size[0] + int(offset / 2), text_y - text_size[1] - int(offset / 2)),\n",
    "            (0, 0, 0),  # rectangle color\n",
    "            cv2.FILLED,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            text,\n",
    "            (text_x, text_y),\n",
    "            font,\n",
    "            font_scale,\n",
    "            (255, 255, 255),  # font color\n",
    "            font_thickness,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "        return frame\n",
    "\n",
    "    clip_marked = clip.fl(add_marker_and_timestamps)\n",
    "    clip_marked.write_videofile(filename, codec=\"libx264\")\n",
    "    clip_marked.close()\n",
    "\n",
    "\n",
    "def make_cmap(number_colors: int, cmap: str = \"cool\"):\n",
    "    color_class = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    C = color_class.to_rgba(np.linspace(0, 1, number_colors))\n",
    "    colors = (C[:, :3] * 255).astype(np.uint8)\n",
    "    return colors\n",
    "\n",
    "\n",
    "def process_and_label_clips(input_video_path, timestamps, clip_length, clip_output_dir, label_output_dir, keypoint_dataframes, confidence_level = 0.8):\n",
    "    # Step 1: Extract clips\n",
    "    extract_clips_ffmpeg_after_reencode(input_video_path, timestamps, clip_length, clip_output_dir)\n",
    "    \n",
    "    # For each timestamp/clip\n",
    "    for idx, start_time in enumerate(timestamps):\n",
    "        # Construct expected clip filename (should match the naming scheme in your extract function)\n",
    "        input_basename_ext = os.path.basename(input_video_path)\n",
    "        input_basename, _ = os.path.splitext(input_basename_ext)\n",
    "        clip_filename = f\"{input_basename}_clip_{idx+1}_{start_time:.2f}s_to_{start_time+clip_length:.2f}s.mp4\"\n",
    "        clip_path = os.path.join(clip_output_dir, clip_filename)\n",
    "        \n",
    "        # Load the clip\n",
    "        clip = VideoFileClip(clip_path)\n",
    "        \n",
    "        # Step 2 & 3: Build xs_arr and ys_arr for the clip\n",
    "        # We assume each dataframe's 'time' column is in seconds relative to the original video.\n",
    "        xs_list = []\n",
    "        ys_list = []\n",
    "        conf_list = []\n",
    "        for key, df in keypoint_dataframes.items():\n",
    "            # Filter the dataframe for the clip’s time window.\n",
    "            # You might need to adjust tolerance if your times are not perfectly aligned.\n",
    "            clip_df = df[(df['time'] >= start_time) & (df['time'] < start_time + clip_length)]\n",
    "            \n",
    "            # Here, we assume one row per frame. \n",
    "            # If the number of rows doesn't match the number of frames in the clip,\n",
    "            # you could resample or interpolate the keypoint positions.\n",
    "            xs_list.append(clip_df['x'].to_numpy())\n",
    "            ys_list.append(clip_df['y'].to_numpy())\n",
    "            conf_list.append(clip_df['confidence'].to_numpy())\n",
    "        \n",
    "        # Convert lists to 2D arrays: each column corresponds to a keypoint.\n",
    "        # (This requires that all keypoint arrays have the same length.)\n",
    "        xs_arr = np.column_stack(xs_list)\n",
    "        ys_arr = np.column_stack(ys_list)\n",
    "        conf_arr = np.column_stack(conf_list)\n",
    "        \n",
    "        # Optional: Verify that xs_arr.shape[0] (number of timepoints) matches expected frame count.\n",
    "        expected_frames = int(clip.fps * clip.duration)\n",
    "        if xs_arr.shape[0] != expected_frames:\n",
    "            print(f\"Warning: Number of keypoint frames ({xs_arr.shape[0]}) does not match video frames ({expected_frames}).\")\n",
    "            # You could add interpolation or padding here if needed.\n",
    "        \n",
    "        # Step 4: Create labeled video for this clip\n",
    "        labeled_clip_filename = f\"{input_basename}_clip_{idx+1}_{start_time:.2f}s_to_{start_time+clip_length:.2f}s_labeled.mp4\"\n",
    "        if not os.path.exists(label_output_dir):\n",
    "            os.makedirs(label_output_dir)\n",
    "        labeled_clip_path = os.path.join(label_output_dir, labeled_clip_filename)\n",
    "\n",
    "        mask_array = conf_arr > confidence_level\n",
    "\n",
    "        create_labeled_video(clip, xs_arr, ys_arr, mask_array=mask_array, filename=labeled_clip_path)\n",
    "        clip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def process_and_label_clips(input_video_path, timestamps, clip_length, clip_output_dir, label_output_dir, keypoint_dataframes, confidence_level = 0.8):\n",
    "    # Step 1: Extract clips\n",
    "    extract_clips_ffmpeg_after_reencode(input_video_path, timestamps, clip_length, clip_output_dir)\n",
    "    \n",
    "    # For each timestamp/clip\n",
    "    for idx, start_time in enumerate(timestamps):\n",
    "        # Construct expected clip filename (should match the naming scheme in your extract function)\n",
    "        input_basename_ext = os.path.basename(input_video_path)\n",
    "        input_basename, _ = os.path.splitext(input_basename_ext)\n",
    "        clip_filename = f\"{input_basename}_clip_{idx+1}_{start_time:.2f}s_to_{start_time+clip_length:.2f}s.mp4\"\n",
    "        clip_path = os.path.join(clip_output_dir, clip_filename)\n",
    "        \n",
    "        # Load the clip\n",
    "        clip = VideoFileClip(clip_path)\n",
    "        \n",
    "        # Step 2 & 3: Build xs_arr and ys_arr for the clip\n",
    "        # We assume each dataframe's 'time' column is in seconds relative to the original video.\n",
    "        xs_list = []\n",
    "        ys_list = []\n",
    "        conf_list = []\n",
    "        for key, df in keypoint_dataframes.items():\n",
    "            # Filter the dataframe for the clip’s time window.\n",
    "            # You might need to adjust tolerance if your times are not perfectly aligned.\n",
    "            clip_df = df[(df['time'] >= start_time) & (df['time'] < start_time + clip_length)]\n",
    "            \n",
    "            # Here, we assume one row per frame. \n",
    "            # If the number of rows doesn't match the number of frames in the clip,\n",
    "            # you could resample or interpolate the keypoint positions.\n",
    "            xs_list.append(clip_df['x'].to_numpy())\n",
    "            ys_list.append(clip_df['y'].to_numpy())\n",
    "            conf_list.append(clip_df['confidence'].to_numpy())\n",
    "        \n",
    "        # Convert lists to 2D arrays: each column corresponds to a keypoint.\n",
    "        # (This requires that all keypoint arrays have the same length.)\n",
    "        xs_arr = np.column_stack(xs_list)\n",
    "        ys_arr = np.column_stack(ys_list)\n",
    "        conf_arr = np.column_stack(conf_list)\n",
    "        \n",
    "        # Optional: Verify that xs_arr.shape[0] (number of timepoints) matches expected frame count.\n",
    "        expected_frames = int(clip.fps * clip.duration)\n",
    "        if xs_arr.shape[0] != expected_frames:\n",
    "            print(f\"Warning: Number of keypoint frames ({xs_arr.shape[0]}) does not match video frames ({expected_frames}).\")\n",
    "            # You could add interpolation or padding here if needed.\n",
    "        \n",
    "        # Step 4: Create labeled video for this clip\n",
    "        labeled_clip_filename = f\"{input_basename}_clip_{idx+1}_{start_time:.2f}s_to_{start_time+clip_length:.2f}s_labeled.mp4\"\n",
    "        if not os.path.exists(label_output_dir):\n",
    "            os.makedirs(label_output_dir)\n",
    "        labeled_clip_path = os.path.join(label_output_dir, labeled_clip_filename)\n",
    "\n",
    "        mask_array = conf_arr > confidence_level\n",
    "\n",
    "        create_labeled_video(clip, xs_arr, ys_arr, mask_array=mask_array, filename=labeled_clip_path)\n",
    "        clip.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of video [s]: 1, recorded at 500.0 fps!\n",
      "Moviepy - Building video /root/capsule/scratch/test_clips/labeled/bottom_camera_clip_1_0.50s_to_1.50s_labeled.mp4.\n",
      "Moviepy - Writing video /root/capsule/scratch/test_clips/labeled/bottom_camera_clip_1_0.50s_to_1.50s_labeled.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/scratch/test_clips/labeled/bottom_camera_clip_1_0.50s_to_1.50s_labeled.mp4\n"
     ]
    }
   ],
   "source": [
    "input_video_path = '/root/capsule/data/BottomViewPylon1-MIB-2025-02-17/inference/behavior_716325_2024-05-31_10-31-14/bottom_camera.mp4'\n",
    "timestamps = [0.5]\n",
    "clip_length = 1.0\n",
    "clip_output_dir = '/root/capsule/scratch/test_clips/clips'\n",
    "label_output_dir = '/root/capsule/scratch/test_clips/labeled'\n",
    "process_and_label_clips(input_video_path, timestamps, clip_length, clip_output_dir, label_output_dir, keypoint_dfs_trimmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import moviepy\n",
    "print(moviepy.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
